---
title: "Decision Tree and Naive Bayes Model Evaluation with Cross-Validation"
author: "Gayathri Sripathy (u1166213)"
date: "January 30, 2018"
output: 
  html_document:
    number_sections: yes
    theme: readable
    highlight: tango
    toc: yes
    fig_width: 15
    fig_height: 10
---

# Set up, Data Import and Preparation
## Loading Packages & import data
```{r - Set up, data import and inspections, warning=FALSE, message=FALSE}

# Load packages after they have been installed
# Package loading. Install the following packages before running this chunk or knitting this program.

library(C50)
library(e1071)
library(matrixStats)
library(scatterplot3d)
library(caret)
library(rminer)
library(rmarkdown)
library(psych)
library(knitr)
library(scatterplot3d)

# Set working directory and import CD_additional_balanced.csv file
setwd("C:/Users/gayat/Desktop/Data Mining/Assignment 2")
bank <- read.csv(file = "CD_additional_modified.csv", stringsAsFactors = TRUE)

#Examine the overall data frame
str(bank)
summary(bank)

#Removing Duration variable and savin the remainder of the data frame as CD_prediction
CD_prediction <- bank[,-c(11)]

#Examine the overall data frame after removing the duration dataframe
str(CD_prediction)
summary(CD_prediction)
```

## Data Partition
```{r - Data Partition, warning=FALSE, message=FALSE}
#Partitioning the data using the createDataPartition() function in caret package. Assigning 70% data to train and 30% data to test
set.seed(100)
inTrain <- createDataPartition(CD_prediction$y, p=0.7, list=FALSE)
str(inTrain)
CDTrain <- CD_prediction[inTrain,]
CDTest <- CD_prediction[-inTrain,]
```

## Distribution after Data Partition
```{r - Distribution in train and test sets, warning=FALSE, message=FALSE}
# Viewing the distributions (in percentages) of the target variable in the CD_prediction data frame, the train set and the test set.

table(CDTrain$y)
table(CDTest$y)

# Using prop.table to generate the conditional probability table

prop.table(table(CDTrain$y))
prop.table(table(CDTest$y))
```

# Simple Decision Tree Training and Testing
## Training a C5.0 model using the default setting
```{r - Training a C5.0 model using the default setting, warning=FALSE, message=FALSE}
# Training a C5.0 model using the default setting.
CD_m1_c50 <- C5.0(y~., CDTrain)
CD_m1_c50
#plot(CD_m1_c50)
summary(CD_m1_c50)
# With the default setting, Size of the decision tree is 23 (that is, the number of non-empty leaves on the tree) and 235 errors (8.1% of cases misclassified)

#Predict() and mmetric() functions
# Evaluating CD_m1_c50
predicted_CD_test1 <- predict(CD_m1_c50, CDTest)
# Generating confusion matrix (3rd argument) based on the true target variable values (1st argument) and the predicted target variable values (2nd argument) using mmetric()
mmetric(CDTest$y, predicted_CD_test1, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CDTest$y, predicted_CD_test1, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
predicted_CD_train1 <- predict(CD_m1_c50, CDTrain)
mmetric(CDTrain$y, predicted_CD_train1, metric="CONF")
mmetric(CDTrain$y, predicted_CD_train1, metric=c("ACC","TPR","PRECISION","F1"))
```

## Reducing the tree complexity by lowering CF levels
```{r - Reducing the tree complexity, warning=FALSE, message=FALSE}

#Change the CF(confidenceFactor) value to prune or unprune a tree
CD_m2_c50 <- C5.0(y~., CDTrain, control = C5.0Control(CF = 0.22))
plot(CD_m2_c50)
summary(CD_m2_c50)

#Predict() and mmetric() functions
## Applying the model to the hold-out test set and generating holdout evaluation metrics
predicted_CD_test2 <- predict(CD_m2_c50, CDTest)
# Generating confusion matrix (3rd argument) based on the true target variable values (1st argument) and the predicted target variable values (2nd argument) using mmetric()
mmetric(CDTest$y, predicted_CD_test2, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CDTest$y, predicted_CD_test2, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
predicted_CD_train2 <- predict(CD_m2_c50, CDTrain)
mmetric(CDTrain$y, predicted_CD_train2, metric="CONF")
mmetric(CDTrain$y, predicted_CD_train2, metric=c("ACC","TPR","PRECISION","F1"))
```

# Simple Na?ve Bayes Model Training and Testing
## Train a naive Bayes model
```{r - Train a naive Bayes model, warning=FALSE, message=FALSE}
# Training a simple Naive based model
CD_m1_nb <- naiveBayes(y~., CDTrain)
CD_m1_nb

# Apply the model to the hold-out test set and generate holdout evaluation metrics
predicted_CD_test1 <- predict(CD_m1_nb, CDTest)
# Generating confusion matrix (3rd argument) based on the true target variable values (1st argument) and the predicted target variable values (2nd argument) using mmetric()
mmetric(CDTest$y, predicted_CD_test1, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CDTest$y, predicted_CD_test1, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
predicted_CD_train1 <- predict(CD_m1_nb, CDTrain)
mmetric(CDTrain$y, predicted_CD_train1, metric="CONF")
mmetric(CDTrain$y, predicted_CD_train1, metric=c("ACC","TPR","PRECISION","F1"))
```

## Exploring Naive Bayes model by removing Previous variable 
```{r - Exploring Naive Bayes model by removing Previous variable, warning=FALSE, message=FALSE}
# Removing the Previous variable and applying Naive Bayes model
CD_m2_nb <- naiveBayes(CDTrain[-c(13,20)], CDTrain$y)
CD_m2_nb

# Apply the model to the hold-out test set and generate holdout evaluation metrics
predicted_CD_test2 <- predict(CD_m2_nb, CDTest)
# Generating confusion matrix (3rd argument) based on the true target variable values (1st argument) and the predicted target variable values (2nd argument) using mmetric()
mmetric(CDTest$y, predicted_CD_test2, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CDTest$y, predicted_CD_test2, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
predicted_CD_train2 <- predict(CD_m2_nb, CDTrain)
mmetric(CDTrain$y, predicted_CD_train2, metric="CONF")
mmetric(CDTrain$y, predicted_CD_train2, metric=c("ACC","TPR","PRECISION","F1"))
```

# Create a Named Cross-validation Function - cv_function
## CV_function arguments
```{r - CV_function arguments, warning=FALSE, message=FALSE}
# set input values to the function(x)
# df =  the whole data set
# target = the column index of the target variable
# nFolds = the number of folds
# classification = the algorithm, e.g. C5.0 or naiveBayes
# seed_value = input for set.seed()
df <- CD_prediction
target <- 20
nFolds <- 3
seedVal <- 500
assign("classification", naiveBayes)
# create folds using the assigned values
set.seed(seedVal)
folds = createFolds(df[,target],nFolds)
```

## CV_function
### Overall accuracy, and the precision, true positive rate and f-measure, Mean values and standard deviations of each performance metric, Performance metrics by fold using Kable() function
```{r - CV_function, warning=FALSE, message=FALSE}
cv_function <- function(df, target, nFolds, seedVal, classification, metrics_list)
{
  set.seed(seedVal)
  folds = createFolds(df[,target],nFolds) 
  
  cv_results <- lapply(folds, function(x)
  { 
    train <- df[-x,-target]
    test  <- df[x,-target]
    train_target <- df[-x,target]
    test_target <- df[x,target]
    # Based on the type of classification assigned the function classifies the data
    classification_model <- classification(train,train_target) 
    # Apply the model to the hold-out test set and generate holdout evaluation metrics
    pred<- predict(classification_model,test)
    # Generating the accuracy, precision, recall, and F values
    return(mmetric(test_target,pred,c("ACC","PRECISION","TPR","F1")))
  })
  #Computing the mean values and standard deviations of each performance metric over all of the folds
  cv_results_m <- as.matrix(as.data.frame(cv_results))
  cv_mean<- as.matrix(rowMeans(cv_results_m))
  colnames(cv_mean) <- "Mean"
  cv_sd <- as.matrix(rowSds(cv_results_m))
  colnames(cv_sd) <- "Sd"
  cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
  #Using kable() to show the performance metrics by fold and their mean values and standard deviations
  kable(cv_all,digits=2)
}
```

# 5 - 5-fold and 10-fold C5.0 and naive Bayes evaluation
## 5 - 5-fold and 10-fold C5.0 and naive Bayes evaluation
```{r - 5-fold and 10-fold C5.0 and naive Bayes evaluation, warning=FALSE, message=FALSE}
df <- CD_prediction
target <- 20
nFolds <- 5
seedVal <- 500
assign("classification", naiveBayes)
metrics_list <- c("ACC","PRECISION","TPR","F1")
cv_function(df, target, nFolds, seedVal, classification, metrics_list)
{
  set.seed(seedVal)
  folds = createFolds(df[,target],nFolds) 
  
  cv_results <- lapply(folds, function(x)
  { 
    train <- df[-x,-target]
    test  <- df[x,-target]
    train_target <- df[-x,target]
    test_target <- df[x,target]
    # Based on the type of classification assigned the function classifies the data
    classification_model <- classification(train,train_target) 
    # Apply the model to the hold-out test set and generate holdout evaluation metrics
    pred<- predict(classification_model,test)
    # Generating the accuracy, precision, recall, and F values
    return(mmetric(test_target,pred,c("ACC","PRECISION","TPR","F1")))
  })
  #Computing the mean values and standard deviations of each performance metric over all of the folds
  cv_results_m <- as.matrix(as.data.frame(cv_results))
  cv_mean<- as.matrix(rowMeans(cv_results_m))
  colnames(cv_mean) <- "Mean"
  cv_sd <- as.matrix(rowSds(cv_results_m))
  colnames(cv_sd) <- "Sd"
  cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
  #Using kable() to show the performance metrics by fold and their mean values and standard deviations
  kable(cv_all,digits=2)
}

# 10 fold
nFolds <- 10
cv_function(df, target, nFolds, seedVal, classification, metrics_list)
{
  set.seed(seedVal)
  folds = createFolds(df[,target],nFolds) 
  
  cv_results <- lapply(folds, function(x)
  { 
    train <- df[-x,-target]
    test  <- df[x,-target]
    train_target <- df[-x,target]
    test_target <- df[x,target]
    # Based on the type of classification assigned the function classifies the data
    classification_model <- classification(train,train_target) 
    # Apply the model to the hold-out test set and generate holdout evaluation metrics
    pred<- predict(classification_model,test)
    # Generating the accuracy, precision, recall, and F values
    return(mmetric(test_target,pred,c("ACC","PRECISION","TPR","F1")))
  })
  #Computing the mean values and standard deviations of each performance metric over all of the folds
  cv_results_m <- as.matrix(as.data.frame(cv_results))
  cv_mean<- as.matrix(rowMeans(cv_results_m))
  colnames(cv_mean) <- "Mean"
  cv_sd <- as.matrix(rowSds(cv_results_m))
  colnames(cv_sd) <- "Sd"
  cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
  #Using kable() to show the performance metrics by fold and their mean values and standard deviations
  kable(cv_all,digits=2)
}

# Different classification algorithm - C5.0
# 5 fold

nFolds <- 5
assign("classification", C5.0)
cv_function(df, target, nFolds, seedVal, classification, metrics_list)
{
  set.seed(seedVal)
  folds = createFolds(df[,target],nFolds) 
  
  cv_results <- lapply(folds, function(x)
  { 
    train <- df[-x,-target]
    test  <- df[x,-target]
    train_target <- df[-x,target]
    test_target <- df[x,target]
    # Based on the type of classification assigned the function classifies the data
    classification_model <- classification(train,train_target) 
    # Apply the model to the hold-out test set and generate holdout evaluation metrics
    pred<- predict(classification_model,test)
    # Generating the accuracy, precision, recall, and F values
    return(mmetric(test_target,pred,c("ACC","PRECISION","TPR","F1")))
  })
 #Computing the mean values and standard deviations of each performance metric over all of the folds
  cv_results_m <- as.matrix(as.data.frame(cv_results))
  cv_mean<- as.matrix(rowMeans(cv_results_m))
  colnames(cv_mean) <- "Mean"
  cv_sd <- as.matrix(rowSds(cv_results_m))
  colnames(cv_sd) <- "Sd"
  cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
  #Using kable() to show the performance metrics by fold and their mean values and standard deviations
  kable(cv_all,digits=2)
}

#10 fold

# 10 fold
nFolds <- 10
cv_function(df, target, nFolds, seedVal, classification, metrics_list)
{
  set.seed(seedVal)
  folds = createFolds(df[,target],nFolds) 
  
  cv_results <- lapply(folds, function(x)
  { 
    train <- df[-x,-target]
    test  <- df[x,-target]
    train_target <- df[-x,target]
    test_target <- df[x,target]
    # Based on the type of classification assigned the function classifies the data
    classification_model <- classification(train,train_target) 
    # Apply the model to the hold-out test set and generate holdout evaluation metrics
    pred<- predict(classification_model,test)
    # Generating the accuracy, precision, recall, and F values
    return(mmetric(test_target,pred,c("ACC","PRECISION","TPR","F1")))
  })
  #Computing the mean values and standard deviations of each performance metric over all of the folds
  cv_results_m <- as.matrix(as.data.frame(cv_results))
  cv_mean<- as.matrix(rowMeans(cv_results_m))
  colnames(cv_mean) <- "Mean"
  cv_sd <- as.matrix(rowSds(cv_results_m))
  colnames(cv_sd) <- "Sd"
  cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
  #Using kable() to show the performance metrics by fold and their mean values and standard deviations
  kable(cv_all,digits=2)
}
```