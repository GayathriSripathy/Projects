---
title: "Video Game Sales Prediction using lm, rpart and M5P"
author: "Gayathri Sripathy (u1166213)"
date: "February 6, 2018"
output: 
  html_document:
    number_sections: yes
    theme: readable
    highlight: tango
    toc: yes
    fig_width: 15
    fig_height: 10
---

```{r - Variables}
#Target variable: Global_Sales

# Other Variables: 
# Name:  the video game name  Do not use the Name column in any model building. 
# Platform: video game platform
# Genre: category of video game
# Critic_score: aggregate score compiled by Metacritic (a website that aggregates reviews of media products)
# Critic_count: The number of critics used in coming up with the Critic_score
# User_score:  Score by Metacritic's subscribers
# User_count: Number of users who gave the user_score
# Rating:  The player age and content ratings
```
# Set up, data import, data exploration, data partitioning and inspection code
## Loading Packages & data import
```{r - Set up, data import and inspections, warning=FALSE, message=FALSE}

# Load packages after they have been installed
# Package loading. Install the following packages before running this chunk or knitting this program.

# Rmarkdown, rpart, rweka, caret, rminer, matrixStats, psych and knitr packages
library(rmarkdown)
library(rpart)
library(RWeka)
library(caret)
library(rminer)
library(matrixStats)
library(knitr)
library(psych)
library(rpart.plot)

# Set working directory and import CD_additional_balanced.csv file
#setwd("C:/Users/gayat/Desktop/Data Mining/Assignment 3")
sales <- read.csv(file = "sales_filtered(1).csv", stringsAsFactors = TRUE)
# Overall inspection
str(sales)
summary(sales)
```

## Exploring numeric and factor variables
```{r - Exploring numeric and factor variables, warning=FALSE, message=FALSE}
# histogram of global sales
hist(sales$Global_Sales)

# histogram of Critic Score
hist(sales$Critic_Score)

# histogram of Critic_Count
hist(sales$Critic_Count)

# histogram of User_Score
hist(sales$User_Score)

# histogram of User_Count
hist(sales$User_Count)

# correlations
# exploring relationships among features: correlation matrix
cor(sales[c("Critic_Score", "Critic_Count", "User_Score", "User_Count")])
# visualizing correlations
pairs.panels(sales)
```

## Linear Regression Model
```{r - Linear Regression Model, warning=FALSE, message=FALSE}
#Removing Name column from the dataset
sales <- sales[,-c(1)]
#Simple lm model
sales_base_model <- lm(Global_Sales ~ ., data = sales)
sales_base_model
summary(sales_base_model)
```

## Data Partitioning
```{r - Data Partitioning, warning=FALSE, message=FALSE}
#Partitioning data in 70% train and 30% test
set.seed(500)
inTrain <- createDataPartition(y=sales$Global_Sales, p = 0.70, list=FALSE)
train_target <- sales[inTrain,3]
test_target <- sales[-inTrain,3]
train_input <- sales[inTrain,-3]
test_input <- sales[-inTrain,-3]
```

## Overall Summaries
```{r - Overall Summaries, warning=FALSE, message=FALSE}
summary(train_target)
summary(test_target)
summary(train_input)
summary(test_input)
```

# lm, rpart and M5P model training and testing
## Training three models using lm, rpart, and M5P on the training set 
```{r - Training three models using lm, rpart, and M5P on the training set, warning=FALSE, message=FALSE}
# Build a lm model using training dataset
sales_base_train_model <- lm(train_target~., data = train_input)
sales_base_train_model
# More detail about the estimated beta coefficients
summary(sales_base_model)
summary(sales_base_train_model)

# Training a model using rpart
# regression tree using rpart
sales_rpart_model <- rpart(train_target ~ ., data = train_input)
sales_rpart_model
# get more detailed information about the tree
summary(sales_rpart_model)
# using the rpart.plot package to create a visualization - a basic decision tree diagram
rpart.plot(sales_rpart_model, digits = 3)
# a few adjustments to the diagram
#The fallen.leaves parameter forces the leaf nodes to be aligned at the bottom of the plot, while the type and extra parameters affect the way the decisions and nodes are labeled
# rpart.plot(ins_rpart_model, digits = 4, fallen.leaves = TRUE, type = 3, extra = 101)

# Training an M5P Model Tree
sales_m5p_model <- M5P(train_target ~ ., data = train_input)
# display the tree
sales_m5p_model
# generate the summary of the model
summary(sales_m5p_model)
```

## Prediction on the Test Data Based on the Trained Model
```{r Apply predictive model, warning=FALSE, message=FALSE}
#lm model prediction
predictions_base_test <- predict(sales_base_train_model, test_input)

# compare the correlation between acutal and predicted expenses in test data
summary(predictions_base_test)
summary(test_target)

# compare the correlation
cor(predictions_base_test, test_target)

# compare the correlation between actual and predicted expneses in training data
predictions_base_train <- predict(sales_base_train_model, train_input)
cor(predictions_base_train, train_target)

# Generating multiple prediction evaluation metrics using rminer package
# performance of predictions on testing data 
mmetric(test_target,predictions_base_test,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
# performance of predictions on training data
mmetric(train_target,predictions_base_train,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))

# rpart model prediction
# generate predictions for the testing and training dataset
predictions_rpart_test <- predict(sales_rpart_model, test_input)
predictions_rpart_train <- predict(sales_rpart_model, train_input)
# compare the distribution of predicted values vs. actual values in testing data
summary(predictions_rpart_test)
summary(test_target)
# compare the correlation between actual and predicted expenses in testing data
cor(predictions_rpart_test,test_target)
# compare the distribution of predicted values vs. actual values in training data
summary(predictions_rpart_train)
summary(train_target)
# compare the correlation
cor(predictions_rpart_train,train_target)

# Generate Prediction Performance Metrics Using Rminer Package  
# Performance of predictions on test data
mmetric(test_target,predictions_rpart_test,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE","COR", "R2"))
# Performance of predictions on train data
mmetric(train_target,predictions_rpart_train,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE","COR", "R2"))

# generate predictions for the m5p model
predictions_m5p_test <- predict(sales_m5p_model, test_input)
predictions_m5p_train <- predict(sales_m5p_model, train_input)

# compare the distribution of predicted values vs. actual values in testing data
summary(predictions_m5p_test)
summary(test_target)
# compare the correlation between actual and predicted expenses in testing data
cor(predictions_m5p_test,test_target)
# compare the distribution of predicted values vs. actual values in training data
summary(predictions_m5p_train)
summary(train_target)
# compare the correlation
cor(predictions_m5p_train,train_target)

# Generating prediction performance metrics using rminer package
# Performance of predictions on test data
mmetric(test_target,predictions_m5p_test,c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","COR","R2"))
mmetric(train_target,predictions_m5p_train,c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","COR","R2"))
```

# Cross-validation of  lm, rpart, and M5P models
## cross-validation of numeric prediction models
```{r - cross-validation of numeric prediction models, warning=FALSE, message=FALSE}
# Set up cv parameters
# df identifies the whole data set by its name
# target identifies the target variable by its column index in df
# nFolds indicates the number of folds for cv
# seedVal carries the seed value for random sampling of instances when creating folds
# prediction_method indicates the prediction method - e.g., lm
# metric_list is a list of evaluation metrics that mmetric should generate

df <- sales
target <- 3
nFolds <- 3
seedVal <- 500
prediction_method <- lm
# This is the same as above: assign("prediction_method", lm)
metrics_list <- c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","R2")

# create folds using the assigned values
set.seed(seedVal)
folds = createFolds(df[,target],nFolds)
str(folds)
# elements in folds: folds$Fold1 = folds[[1]], folds$Foldi = folds[[i]]
# length of folds = nFolds

cv_function <- function(df, target, nFolds, seedVal, prediction_method, metrics_list)
{
  # create folds
  set.seed(seedVal)
  folds = createFolds(df[,target],nFolds) 
  # perform cross validation
  cv_results <- lapply(folds, function(x)
  { 
    test_target <- df[x,target]
    test_input  <- df[x,-target]

    train_target <- df[-x,target]
    train_input <- df[-x,-target]

    prediction_model <- prediction_method(train_target~.,train_input) 
    pred<- predict(prediction_model,test_input)
    return(mmetric(test_target,pred,metrics_list))
  })
  # generate means and sds and show cv results, means and sds using kable
  cv_results_m <- as.matrix(as.data.frame(cv_results))
  cv_mean<- as.matrix(rowMeans(cv_results_m))
  cv_sd <- as.matrix(rowSds(cv_results_m))
  colnames(cv_mean) <- "Mean"
  colnames(cv_sd) <- "Sd"
  cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
  knitr::kable(t(cv_all),digits=2)
}

```

## 10-fold cross-validation results of the lm, rpart and M5P models
```{r - 10-fold cross-validation, warning=FALSE, message=FALSE}
#Performing 10 fold cross validation for lm, rpart and m5p models
df <- sales
target <- 3
nFolds <- 3
seedVal <- 500
prediction_method <- lm
# This is the same as above: assign("prediction_method", lm)
metrics_list <- c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","R2")

#lm model
cv_function(df, target, 10, seedVal, lm, metrics_list)

#rpart model
cv_function(df, target, 10, seedVal, rpart, metrics_list)

#m5p model
cv_function(df, target, 10, seedVal, M5P, metrics_list)

```

# Improving the models
## Adding quadratic term of User_Count
```{r - Improve the models, warning=FALSE, message=FALSE}
#Adding a quadratic term
sales$User_Count2 <- sales$User_Count^2

#Partitioning the dataset again
set.seed(500)
inTrain <- createDataPartition(y=sales$Global_Sales , p=0.70, list=FALSE)
train_target <- sales[inTrain,]
test_target <- sales[-inTrain,]
# train_input <- sales[inTrain,-3]
# test_input <- sales[-inTrain,-3]
```

## Model with quadratic term
```{r - Model with quadratic term, warning=FALSE, message=FALSE}
# adding the quadratic term
sales_improved_train_model <- lm(train_target[,3] ~ ., data = train_target[,-c(3)])
summary(sales_improved_train_model)
```

##Cross - Validation of models with User_Count_Squared included
```{r - Cross - Validation of models, message=FALSE, warning=FALSE}
df <- test_target
target <- 3
nFolds <- 10
seedVal <- 500
metrics_list <- c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","R2")

# lm model with user_count_squared
cv_function(df, target, 10, seedVal, lm, metrics_list)

# rpart model with user_count_squared
cv_function(df, target, 10, seedVal, rpart, metrics_list)

# m5p model with user_count_squared
cv_function(df, target, 10, seedVal, M5P, metrics_list)
```

# Improve the models with the log term of User_Count
## Natural log transformation of User_Count
```{r - natural log transformation of User_Count, warning=FALSE, message= FALSE}
#Adding log term to the dataset
sales$log_User_Count <- log(sales$User_Count)

#Partitioning the dataset again
set.seed(500)
inTrain <- createDataPartition(y=sales$Global_Sales , p=0.70, list=FALSE)
train_target <- sales[inTrain,]
test_target <- sales[-inTrain,]
#removing user count and user count squared terms
train_target <- train_target[,-c(7,9)]
test_target <- test_target[,-c(7,9)]
```

## Model with the log term
```{r - Model with log term, warning=FALSE, message=FALSE}
# adding the log term and removing user_count and user_count_squared terms
sales_improved_train_model1 <- lm(train_target[,3] ~ ., data = train_target[,-3])
summary(sales_improved_train_model1)
```
## 10 -fold Cross - Validation of models with log_user_count included
```{r - 10 -fold Cross - Validation of models, message=FALSE, warning=FALSE}
df <- test_target
target <- 3
nFolds <- 10
seedVal <- 500
metrics_list <- c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","R2")

# lm model with user_count_squared
cv_function(df, target, 10, seedVal, lm, metrics_list)

# rpart model with user_count_squared
cv_function(df, target, 10, seedVal, rpart, metrics_list)

# m5p model with user_count_squared
cv_function(df, target, 10, seedVal, M5P, metrics_list)
```