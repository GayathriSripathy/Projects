---
title: "Predicting House Prices"
author: "Gayathri Sripathy (u1166213)"
date: "November 3, 2017"
output: pdf_document
---

```{r setup, include=FALSE, echo=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Hmisc)
library(corrplot)
library(Amelia)
library(mice)
library(ggplot2)
library(lattice)
library(RColorBrewer)
library(caret)
library(arm)
library(dplyr)
library(ggplot2)
library(faraway)
library(MASS)
library(rminer)
library(psych)
library(missForest)
library(moments)
library(VIM)
library(tree)

library(readr)
train1 <- read.csv("C:/Users/gayat/Desktop/Statistics & Predictive Analysis/November 2_Final Project/train.csv")

dim(train1)
str(train1)
summary(train1)

test1 <- read.csv("C:/Users/gayat/Desktop/Statistics & Predictive Analysis/November 2_Final Project/test.csv")

dim(test1)
str(test1)
summary(test1)
```

#Introduction
The goal of the Kaggle competition - House Prices, is to come up with a model that predicts house prices in Ames, IA. The data, has been split into 50% train and 50% test sets respectively with 79 explanatory variables describing (almost) every aspect of residential homes. The objective here is to fit a parsimonious model with 5 predictors, measure its performance on the train set using RMSE, R2 and estimate its performance on the test set using cross-validation.

#Data Modeling and Cleaning
##Observations
* Of the 79 predictor variables, 43 were categorical variables and 36 were numerical variables.
* Since the train and test data sets contained many NA values the data cleaning was done on both the sets.
* For convenience, the train and test sets were combined into one dataset for data cleaning purpose.
* Around 34 variables contained NA values and there were around 13965 NA values.
##Data Distribution
```{r, echo=FALSE, warning=FALSE}
# combining the data sets

#Setting SalesPrice to 0 for all rows of test data.
sale_price <- NULL
for(i in 1:nrow(test1)){
  sale_price[i] <- 0
}

#Sale Price of train data is strongly skewed as can be seen in the below histogram/
hist(train1$SalePrice,xlab="Sale Price",main = "Sale Price")

#log transforming it to get a better distribution
hist(log(train1$SalePrice),xlab="Log Sale Price",main = "Log transformed Sale Price")

#creating a log of the SalePrice
train1$logSalePrice<-log(train1$SalePrice)
test1$SalePrice <- sale_price

#Train data has extra columns compared to test.So let us remove the extra 
#columns while combining 
house_data<-rbind(train1[,-c(dim(train1)[2])],test1)

#MoSold ,MSSubClass,YrSold are continouous variables
#Converting them into categorical variables
house_data$MSSubClass <- as.factor(house_data$MSSubClass)
house_data$MoSold <- as.factor(house_data$MoSold)
house_data$YrSold <- as.factor(house_data$YrSold)
```
##Data Visualization
```{r, echo=FALSE, warning=FALSE}
#Vizualizing the missing data
missers <- subset(house_data[,c("MSZoning","LotFrontage","Alley","Utilities","Exterior1st","Exterior2nd","MasVnrType","MasVnrArea","BsmtQual","BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinSF1","BsmtFinType2","BsmtFinSF2","BsmtUnfSF","TotalBsmtSF","Electrical","BsmtFullBath","BsmtHalfBath","KitchenQual","Functional","FireplaceQu","GarageType","GarageYrBlt","GarageFinish","GarageCars","GarageArea","GarageQual","GarageCond","PoolQC","Fence","MiscFeature","SaleType")])
miceplot2 <- aggr(missers,col=c("gray","black"),
                  numbers=TRUE,combined=TRUE,varheight=TRUE,border=NA,
                  sortVars=TRUE,sortCombs=FALSE,ylabs=c("Missing Data Pattern"),
                  labels=names(missers),cex.axis=.7)

missmap(missers)
```

##Approach
####Converting NA to None for Categorical Variables
```{r, echo=FALSE, warning=FALSE}
#For many predictors, NA does not actually mean missing data but absence of 
#the feature. Eg: Fence NA means no Fence. We impute those NAs as a category
# called "None"

var_conv<-c("Alley","BsmtQual","BsmtCond","BsmtExposure","BsmtFinType1",
            "BsmtFinType2","FireplaceQu","GarageType","GarageFinish",
            "GarageQual","GarageCond","PoolQC","Fence","MiscFeature")

## impute NA to without
without<-function(data,var){
  levels(data[,var]) <- c(levels(data[,var]), "None")
  data[,var][is.na(data[,var])] <- "None"
  return(data[,var])
}
data_combined<-house_data

for(i in 1:length(var_conv)){
  data_combined[,var_conv[i]] <- as.character(data_combined[,var_conv[i]])
}

for (i in 1:length(var_conv)){
  data_combined[,var_conv[i]]<-without(house_data,var_conv[i]) 
}

for(i in 1:length(var_conv)){
  data_combined[,var_conv[i]] <- as.factor(data_combined[,var_conv[i]])
}
#We can see that the NA values have been changed to None
head(data_combined)
```

####Imputation for Variables
* MissForest imputation method has been used to impute missing data in variables
```{r, echo=FALSE, warning=FALSE}
data_combined$GarageYrBlt[is.na(data_combined$GarageYrBlt)]<-0

#Imputation using missforest
mfimp <- missForest(data_combined, maxiter = 1)
str(mfimp)
head(mfimp$ximp)

data_imputed <- mfimp$ximp
```

####Splitting the data post cleaning/Imputation
```{r, echo=FALSE, warning=FALSE}
house_train_clean <- data_imputed[data_imputed$Id < 1461,]
house_test_clean <- data_imputed[data_imputed$Id > 1460,]

#Removing SalePrice from test data
house_test_clean <- house_test_clean[,-81]
summary(house_train_clean)

#RMSE Function
rmse <- function(yhat, y) {
  sqrt((mean((yhat - y)^2)))
}
```

#Model and Predictions
###Variable selection using StepAIC
* Selecting variables using both direction model

```{r, echo=FALSE, warning=FALSE}
stepAIC(lm(log(SalePrice) ~ ., data=house_train_clean, direction  = "both"))

```
####Significant Variables
* Using the above three stepAIC variable detections,the following variables are considered significant
```{r, echo=FALSE, warning=FALSE}
#OverallQual,TotalBsmtSF,bathrooms,GrLivArea,GarageCars,Neighborhood,Condition2,
#BldgType,RoofMatl ,ExterQual,1stFlrSF,2ndFlrS,SaleCondition,PoolArea,GarageArea,
#KitchenAbvGr,BedroomAbvGr
```
##Model with significant variables
* Different combinations of variables were used to predict the performance of the model and these 10 variables provided better results
```{r, echo=FALSE, warning=FALSE}
model_10 <- lm(log(SalePrice) ~ Neighborhood + BldgType + RoofMatl + OverallQual + 
                 OverallCond + YearBuilt + GarageArea + X1stFlrSF + X2ndFlrSF + 
                 BsmtFullBath , data = house_train_clean)

summary(model_10)
```
#Model Performance with interaction terms
```{r, echo=FALSE, warning=FALSE}
model_final <- lm(log(SalePrice) ~Neighborhood + BldgType + RoofMatl + OverallQual + OverallCond * YearBuilt + GarageArea * X1stFlrSF + X2ndFlrSF + BsmtFullBath , data = house_train_clean)

summary(model_final)

model_final_std <- standardize(lm(log(SalePrice) ~Neighborhood + BldgType + 
                                    RoofMatl + OverallQual + OverallCond * YearBuilt +
                                    GarageArea * X1stFlrSF + X2ndFlrSF + BsmtFullBath , 
                                  data = house_train_clean))
```

#In Sample RMSE & R-squared
```{r, echo=FALSE, warning=FALSE}
summary(house_train_clean)
summary(model_final)
rmse(log(house_train_clean$SalePrice), predict(model_final))
rmse(house_train_clean$SalePrice, exp(predict(model_final)))
```

#Out of Sample Performance
```{r, echo=FALSE, warning=FALSE}
set.seed(200)
train(log(SalePrice) ~ Neighborhood + BldgType + RoofMatl + OverallQual + 
        OverallCond * YearBuilt + GarageArea * X1stFlrSF + X2ndFlrSF + 
        BsmtFullBath, data=house_train_clean,
      method= "lm", 
      trControl= trainControl(method="repeatedcv", repeats = 5, number=10))
predict_test <- predict(model_final, house_test_clean)
house_test_clean$SalePrice <- exp(predict_test)

```


#Predicted Values
```{r}
write.csv(file = "C:/Users/gayat/Desktop/Statistics & Predictive Analysis/November 2_Final Project/testfinal.csv", house_test_clean)
```
