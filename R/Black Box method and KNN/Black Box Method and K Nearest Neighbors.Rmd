---
title: "Black Box Method and K Nearest Neighbors"
author: "Gayathri Sripathy"
date: "Feb 26, 2018"
output: 
  html_document:
    number_sections: yes
    theme: readable
    highlight: tango
    toc: yes
    fig_width: 15
    fig_height: 10
---
##  Create and evaluate neural network models for numeric prediction with different number of hidden nodes in a hidden layer
 MLP's default parameter values of MLP,L=0.3,M=0.2, N=500, H='a'

- L: learning rate with default=0.3
- M: momemtum with default=0.2
- N: number of epochs with default=500
- H: \<comma seperated numbers for nodes on each layer\>
  
The hidden nodes to be created on each layer:
  - an integer, or the letters 'a' = (# of attribs + # of classes) / 2, 
  - 'i' = # of attribs, 'o' = # of classes, 't' = (# of attribs + # of classes)
  - default of H is 'a'.

Some of IBk the setting options are:
 
 -I: weigh neighbors by the inverse of their distance  (use when k > 1)
  
 -F: weigh neighbors by 1 - their distance (use when k > 1)

 -K <number of neighbors>: Number of nearest neighbors (k) used in classification. (Default = 1)

 -X: Select the number of nearest neighbors between 1 and the k value specified using hold-one-out evaluation on the training data (use when k > 1)

 -E: Minimize mean squared error rather than mean absolute error when using -X option with numeric prediction. (Not for classification)


# Package load, data import, inspection, and partitioning
## Load Packages
```{r -  Package load, warning=FALSE, message=FALSE}
# Load the following packages. Install them first if necessary.
#Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_161') # for 64-bit version
library(rJava)
library(caret)
library(kernlab)
library(rminer)
library(matrixStats)
library(knitr)
library(RWeka)
```

## Import the sales_filtered.csv and partition the data
### Import data
```{R - Import data, warning=FALSE, message=FALSE}
#Importing sales_filtered data and setting string as factors as FALSE
#setwd("C:/Users/gayat/Desktop/Data Mining/Assignment 4")
sales <- read.csv(file = "sales_filtered (1).csv", stringsAsFactors = FALSE)
str(sales)
```

### Dataframe excluding Name
```{r - Dataframe excluding Name,warning=FALSE, message=FALSE}
#Excluding the variable name and forming a new dataframe
sales_new <- sales[,-1]
str(sales_new)
```

### Transforming character variables into factors
```{r - Transform character variables into factors, warning=FALSE, message=FALSE}
# Transforming character variables Platform, Genre, Rating into factors
sales_new$Platform <- as.factor(sales_new$Platform)
sales_new$Genre <- as.factor(sales_new$Genre)
sales_new$Rating <- as.factor(sales_new$Rating)
str(sales_new)
```

### Create the training and testing sets
```{r - Create the training and testing sets, warning=FALSE, message=FALSE}
#Partitioning data in 70% train and 30% test
set.seed(500)
inTrain <- createDataPartition(y=sales_new$Global_Sales, p = 0.70, list=FALSE)
train_target <- sales_new[inTrain,3]
test_target <- sales_new[-inTrain,3]
train_input <- sales_new[inTrain,-3]
test_input <- sales_new[-inTrain,-3]
```

## ImportCD_additional_modified.csv and partition the dataset
### Importing CD_additional_modified.csv
```{r - Importing CD_additional_modified.csv, warning=FALSE, message=FALSE}
#Importing CD_Additional_modified data and setting string  as factors to TRUE
#setwd("C:/Users/gayat/Desktop/Data Mining/Assignment 4")
CD <- read.csv(file = "CD_additional_modified.csv", stringsAsFactors = TRUE)
str(CD)
```

### Creating a data frame excluding duration
```{r - Creating a data frame excluding duration, warning=FALSE, message=FALSE}
# Creating a data frame excluding duration
CD_new <- CD[,-11]
str(CD_new)
```

### Creating the training and testing sets from CD_new
```{r - Creating the training and testing sets from CD_new, warning=FALSE, message=FALSE}
#Partitioning data in 70% train and 30% test
set.seed(500)
inTrain <- createDataPartition(y=CD_new$y, p = 0.70, list=FALSE)
CD_train <- CD_new[inTrain,]
CD_test <- CD_new[-inTrain,]
```

# Building and evaluating neural network models for numeric prediction and classification
## Building and evaluating MLP models for numeric prediction
### Building an MLP model on MultilayerPerceptron()'s
```{r - Building an MLP model on MultilayerPerceptrons, warning=FALSE, message=FALSE}
# Building an MLP model on MultilayerPerceptron()'s default setting on the video game sales data training set
MLP <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
model_1 <- MLP(train_target ~ .,data = train_input)

# Evaluating the model performance on the training set and testing set.
predictions_base_test <- predict(model_1, test_input)
# compare the correlation between acutal and predicted expenses in test data
summary(predictions_base_test)
summary(test_target)

# compare the correlation
cor(predictions_base_test, test_target)

# compare the correlation between actual and predicted expneses in training data
predictions_base_train <- predict(model_1, train_input)
cor(predictions_base_train, train_target)

# Generating multiple prediction evaluation metrics using rminer package
# performance of predictions on testing data 
mmetric(test_target,predictions_base_test,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
# performance of predictions on training data
mmetric(train_target,predictions_base_train,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
``` 

### Building and evaluating a two-hidden-layer MLP model
```{r - Building and evaluating a two-hidden-layer MLP model, warning=FALSE, message=FALSE}
# Defining learning rate, momentum, no.of epochs and no.of hidden layers
l <- 0.3
m <- 0.2
n <-500
h <- 'a'
model_2 <- MLP(train_target ~ .,data = train_input,control = Weka_control(L=l,M=m, N=n,H=2))
model_2

#Evaluating the model performance on the training set and testing set
predictions_h0_test <- predict(model_2, test_input)
# compare the correlation between acutal and predicted expenses in test data
summary(predictions_h0_test)
summary(test_target)

# compare the correlation
cor(predictions_h0_test, test_target)

# compare the correlation between actual and predicted expneses in training data
predictions_h0_train <- predict(model_2, train_input)
cor(predictions_h0_train, train_target)

# Generating multiple prediction evaluation metrics using rminer package
# performance of predictions on testing data 
mmetric(test_target,predictions_h0_test,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
# performance of predictions on training data
mmetric(train_target,predictions_h0_train,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
```

## Building and evaluating MLP models for classification with the bank telemarketing data
### Build an MLP model on MultilayerPerceptron()'s default setting on the training set of bank data
```{r - Building and evaluating MLP models for classification with the bank telemarketing data,  warning=FALSE, message=FALSE}
# Model with default setting
model_3 <- MLP(y ~ .,data = CD_train)
model_3

#Evaluating the model performance on the training set and testing set
predictions_CD_test <- predict(model_3, CD_test)
# compare the correlation between acutal and predicted expenses in test data
summary(predictions_CD_test)

#Training Set
predictions_CD_train <- predict(model_3, CD_train)

#evaluation metrics
mmetric(CD_test$y, predictions_CD_test, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CD_test$y, predictions_CD_test, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
predicted_CD_train <- predict(model_3, CD_train)
mmetric(CD_train$y, predicted_CD_train, metric="CONF")
mmetric(CD_train$y, predicted_CD_train, metric=c("ACC","TPR","PRECISION","F1"))
```

### Building and evaluating a two-hidden-layer MLP model with bank data
```{r - Building and evaluating a two-hidden-layer MLP model with bank data, warning=FALSE, message=FALSE}
# Defining learning rate, momentum, no.of epochs and no.of hidden layers

l <- 0.3
m <- 0.2
n <-500
h <- 'a'
model_b2 <- MLP(y ~ .,data = CD_train,control = Weka_control(L=l,M=m, N=n,H=2))
model_b2

#Evaluating the model performance on the training set and testing set
predictions_bh0_test <- predict(model_b2, CD_test)
summary(predictions_bh0_test)

# For comparison, applying the model to the train set and generating evaluation metrics. 
predictions_bh0_train <- predict(model_b2, CD_train)

#evaluation metrics
mmetric(CD_test$y, predictions_bh0_test, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CD_test$y, predictions_bh0_test, metric=c("ACC","TPR","PRECISION","F1"))

# Checking the performance drop in the holdout set.
mmetric(CD_train$y, predictions_bh0_train, metric="CONF")
mmetric(CD_train$y, predictions_bh0_train, metric=c("ACC","TPR","PRECISION","F1"))
```

# Building and evaluating SVM (ksvm) models for numeric prediction and classification
## Build and evaluate ksvm models for numeric predictionwith the video game sales data
### Build a model on ksvm()'s default setting on the training set
```{r - Build a model on ksvm default setting on the training set of video game sales data, warning=FALSE,message=FALSE}
# Default ksvm model
set.seed(500)
ksvm_video_default <- ksvm(train_target ~ .,data = train_input)

#Evaluating the model performance on the training set and testing set
predictions_ksvm_test1 <- predict(ksvm_video_default, test_input)
summary(predictions_ksvm_test1)

# training data
predictions_ksvm_train1 <- predict(ksvm_video_default, train_input)

# Generating multiple prediction evaluation metrics using rminer package
# performance of predictions on testing data 
mmetric(test_target,predictions_ksvm_test1,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
# performance of predictions on training data
mmetric(train_target,predictions_ksvm_train1,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
```

### Build a ksvm model using a different kernel function
```{r - Build a ksvm model using a different kernel function, warning=FALSE,message=FALSE}
# ksvm model using a different kernel function
set.seed(500)
ksvm_video_laplacedot <- ksvm(train_target ~ .,data = train_input, kernel="laplacedot")

#Evaluating the model performance on the training set and testing set
predictions_ksvm_test2 <- predict(ksvm_video_laplacedot, test_input)
summary(predictions_ksvm_test2)

# training data
predictions_ksvm_train2 <- predict(ksvm_video_laplacedot, train_input)

# Generating multiple prediction evaluation metrics using rminer package
# performance of predictions on testing data 
mmetric(test_target,predictions_ksvm_test2,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
# performance of predictions on training data
mmetric(train_target,predictions_ksvm_train2,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
```

### Build a ksvm model using a different cost value
```{r - Build a ksvm model using a different cost value, message=FALSE,warning=FALSE}
# ksvm model using a different cost value
set.seed(500)
ksvm_video_laplacedot_C20 <- ksvm(train_target ~ .,data = train_input, kernel="laplacedot",C=20)

#Evaluating the model performance on the training set and testing set
predictions_ksvm_test3 <- predict(ksvm_video_laplacedot_C20, test_input)
summary(predictions_ksvm_test2)

# training data
predictions_ksvm_train3 <- predict(ksvm_video_laplacedot_C20, train_input)

# Generating multiple prediction evaluation metrics using rminer package
# performance of predictions on testing data 
mmetric(test_target,predictions_ksvm_test3,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
# performance of predictions on training data
mmetric(train_target,predictions_ksvm_train3,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
```

## Build and evaluate ksvm models for classification with the bank telemarketing data
### Build a model on ksvm()'s default setting
```{r - Build a model on ksvm default setting, warning=FALSE,message=FALSE}
#Default ksvm model
set.seed(500)
ksvm_bank_default <- ksvm(y ~ .,data = CD_train)

#Evaluating the model performance on the training set and testing set
predictions_CD_test <- predict(ksvm_bank_default, CD_test)
# compare the correlation between acutal and predicted expenses in test data
summary(predictions_CD_test)

#Training Set
predictions_CD_train <- predict(ksvm_bank_default, CD_train)

#evaluation metrics
mmetric(CD_test$y, predictions_CD_test, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CD_test$y, predictions_CD_test, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
predicted_CD_train <- predict(ksvm_bank_default, CD_train)
mmetric(CD_train$y, predicted_CD_train, metric="CONF")
mmetric(CD_train$y, predicted_CD_train, metric=c("ACC","TPR","PRECISION","F1"))
```

### Build a ksvm model using a different kernel function on bank data
```{r - Build a ksvm model using a different kernel function on bank data, warning=FALSE, message=FALSE}
# ksvm model using a different kernel function on bank data
set.seed(500)
ksvm_bank_laplacedot <- ksvm(y ~ .,data = CD_train, kernel="laplacedot")

#Evaluating the model performance on the training set and testing set
predictions_CD_test <- predict(ksvm_bank_laplacedot, CD_test)
# compare the correlation between acutal and predicted expenses in test data
summary(predictions_CD_test)

#Training Set
predictions_CD_train <- predict(ksvm_bank_laplacedot, CD_train)

#evaluation metrics
mmetric(CD_test$y, predictions_CD_test, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CD_test$y, predictions_CD_test, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
predicted_CD_train <- predict(ksvm_bank_laplacedot, CD_train)
mmetric(CD_train$y, predicted_CD_train, metric="CONF")
mmetric(CD_train$y, predicted_CD_train, metric=c("ACC","TPR","PRECISION","F1"))
```

### Build a ksvm model using a different cost value on bank data
```{r - Build a ksvm model using a different cost value on bank data, warning=FALSE, message=FALSE}
# ksvm model using a different cost value on bank data
set.seed(500)
ksvm_bank_laplacedot_c20 <- ksvm(y ~ .,data = CD_train, kernel="laplacedot", C=20)

#Evaluating the model performance on the training set and testing set
predictions_CD_test <- predict(ksvm_bank_laplacedot_c20, CD_test)
# compare the correlation between acutal and predicted expenses in test data
summary(predictions_CD_test)

#Training Set
predictions_CD_train <- predict(ksvm_bank_laplacedot_c20, CD_train[,-20])

#evaluation metrics
mmetric(CD_test$y, predictions_CD_test, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CD_test$y, predictions_CD_test, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
predicted_CD_train <- predict(ksvm_bank_laplacedot_c20, CD_train)
mmetric(CD_train$y, predicted_CD_train, metric="CONF")
mmetric(CD_train$y, predicted_CD_train, metric=c("ACC","TPR","PRECISION","F1"))
```

# Build and evaluate knn (IBk) models for numeric prediction and classification
## Build and evaluate IBk models for numeric prediction with the video game sales data
### Build a model on IBk default setting
```{r - Build a model on IBk default setting, warning=FALSE, message=FALSE}
#Build a model on IBk()'s default setting on the training set. 
knn_model <- IBk(train_target ~ .,data = train_input,control = Weka_control(K=1))
knn_model

#Evaluating the model performance on the training set and testing set
predictions_knn_test1 <- predict(knn_model, test_input)
summary(predictions_knn_test1)

# training data
predictions_knn_train1 <- predict(knn_model, train_input)

# Generating multiple prediction evaluation metrics using rminer package
# performance of predictions on testing data 
mmetric(test_target,predictions_knn_test1,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
# performance of predictions on training data
mmetric(train_target,predictions_knn_train1,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
```
### Build a model on IBk using different k value
```{r - Build a model on IBk using different k value, warning=FALSE, message=FALSE}
#Build a model on IBk using different k value 
knn_model1 <- IBk(train_target ~ .,data = train_input,control = Weka_control(K=5))
knn_model1

#Evaluating the model performance on the training set and testing set
predictions_knn_test1 <- predict(knn_model1, test_input)
summary(predictions_knn_test1)

# training data
predictions_knn_train1 <- predict(knn_model1, train_input)

# Generating multiple prediction evaluation metrics using rminer package
# performance of predictions on testing data 
mmetric(test_target,predictions_knn_test1,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
# performance of predictions on training data
mmetric(train_target,predictions_knn_train1,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
```

### Build a model on IBk using a weighted voting approach
```{r - Build a model on IBk using a weighted voting approach, warning=FALSE, message=FALSE}
#Build a model on IBk using a weighted voting approach 
knn_model2 <- IBk(train_target ~ .,data = train_input,control = Weka_control(K=5, I=TRUE))
knn_model2

#Evaluating the model performance on the training set and testing set
predictions_knn_test1 <- predict(knn_model2, test_input)
summary(predictions_knn_test1)

# training data
predictions_knn_train1 <- predict(knn_model2, train_input)

# Generating multiple prediction evaluation metrics using rminer package
# performance of predictions on testing data 
mmetric(test_target,predictions_knn_test1,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
# performance of predictions on training data
mmetric(train_target,predictions_knn_train1,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
```

### Build a model on IBk by automatically selecting K
```{r - Build a model on IBk by automatically selecting K, warning=FALSE, message=FALSE}
#Build a model on IBk by automatically selecting K
knn_model2 <- IBk(train_target ~ .,data = train_input, control = Weka_control(K=100, X=TRUE))
knn_model2

#Evaluating the model performance on the training set and testing set
predictions_knn_test1 <- predict(knn_model2, test_input)
summary(predictions_knn_test1)

# training data
predictions_knn_train1 <- predict(knn_model2, train_input)

# Generating multiple prediction evaluation metrics using rminer package
# performance of predictions on testing data 
mmetric(test_target,predictions_knn_test1,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
# performance of predictions on training data
mmetric(train_target,predictions_knn_train1,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
```

## Build and evaluate IBk models for classification with the bank telemarketing data
### Build a model on IBk()'s default setting on bank data
```{r - Build a model on IBk default setting on bank data, warning=FALSE,message=FALSE}
# Build a model on IBk()'s default setting on bank data
knn_model2 <- IBk(y ~ .,data = CD_train, control = Weka_control(K=1))
knn_model2

#Evaluating the model performance on the training set and testing set
predictions_CD_test <- predict(knn_model2, CD_test)
# compare the correlation between acutal and predicted expenses in test data
summary(predictions_CD_test)

#Training Set
predictions_CD_train <- predict(knn_model2, CD_train)

#evaluation metrics
mmetric(CD_test$y, predictions_CD_test, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CD_test$y, predictions_CD_test, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
mmetric(CD_train$y, predicted_CD_train, metric="CONF")
mmetric(CD_train$y, predicted_CD_train, metric=c("ACC","TPR","PRECISION","F1"))
```

### Build a model on IBk using different k value on bank data
```{r - Build a model on IBk using different k value on bank data, warning=FALSE,message=FALSE}
# Model on IBk using different k value on bank data
knn_model2 <- IBk(y ~ .,data = CD_train, control = Weka_control(K=5))
knn_model2

#Evaluating the model performance on the training set and testing set
predictions_CD_test <- predict(knn_model2, CD_test)
# compare the correlation between acutal and predicted expenses in test data
summary(predictions_CD_test)

#Training Set
predictions_CD_train <- predict(knn_model2, CD_train)

#evaluation metrics
mmetric(CD_test$y, predictions_CD_test, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CD_test$y, predictions_CD_test, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
mmetric(CD_train$y, predicted_CD_train, metric="CONF")
mmetric(CD_train$y, predicted_CD_train, metric=c("ACC","TPR","PRECISION","F1"))
```

### Build a model on IBk using a weighted voting approach on bank data
```{r - Build a model on IBk using a weighted voting approach on bank data, warning=FALSE,message=FALSE}
# Model on IBk using a weighted voting approach on bank data
knn_model2 <- IBk(y ~ .,data = CD_train, control = Weka_control(K=5, I=TRUE))
knn_model2

#Evaluating the model performance on the training set and testing set
predictions_CD_test <- predict(knn_model2, CD_test)
# compare the correlation between acutal and predicted expenses in test data
summary(predictions_CD_test)

#Training Set
predictions_CD_train <- predict(knn_model2, CD_train)

#evaluation metrics
mmetric(CD_test$y, predictions_CD_test, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CD_test$y, predictions_CD_test, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
mmetric(CD_train$y, predicted_CD_train, metric="CONF")
mmetric(CD_train$y, predicted_CD_train, metric=c("ACC","TPR","PRECISION","F1"))
```

### Build a model on IBk by automatically selecting K on bank data
```{r - Build a model on IBk by automatically selecting K on bank data, warning=FALSE,message=FALSE}
# Model on IBk by automatically selecting K on bank data
knn_model2 <- IBk(y ~ .,data = CD_train, control = Weka_control(K=100, X=TRUE, I=TRUE))
knn_model2

#Evaluating the model performance on the training set and testing set
predictions_CD_test <- predict(knn_model2, CD_test)
# compare the correlation between acutal and predicted expenses in test data
summary(predictions_CD_test)

#Training Set
predictions_CD_train <- predict(knn_model2, CD_train)

#evaluation metrics
mmetric(CD_test$y, predictions_CD_test, metric="CONF")
# Generating the accuracy, precision, recall, and F values
mmetric(CD_test$y, predictions_CD_test, metric=c("ACC","TPR","PRECISION","F1"))

# For comparison, applying the model to the train set and generating evaluation metrics. 
# Checking the performance drop in the holdout set.
mmetric(CD_train$y, predicted_CD_train, metric="CONF")
mmetric(CD_train$y, predicted_CD_train, metric=c("ACC","TPR","PRECISION","F1"))
```

# Cross-validation function for classification and numeric prediction
## Define a named function 
```{r - Define a named function, warning=FALSE,message=FALSE}
#Cross-validation function for MLP
seedVal <- 500
metrics_list <- c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","R2")
cv_function_MLP <- function(df, target, nFolds, seedVal, metrics_list, l, m, n, h)
{
# create folds using the assigned values

set.seed(seedVal)
folds = createFolds(df[,target],nFolds)

# The lapply loop

cv_results <- lapply(folds, function(x)
{ 
# data preparation:

  test_target <- df[x,target]
  test_input <- df[x,-target]
  
  train_target <- df[-x,target]
  train_input <- df[-x,-target]
  pred_model <- MLP(train_target ~ .,data = train_input,control = Weka_control(L=l,M=m, N=n,H=h))  
  pred <- predict(pred_model, test_input)
  return(mmetric(test_target,pred,metrics_list))
})

cv_results_m <- as.matrix(as.data.frame(cv_results))
cv_mean<- as.matrix(rowMeans(cv_results_m))
cv_sd <- as.matrix(rowSds(cv_results_m))
colnames(cv_mean) <- "Mean"
colnames(cv_sd) <- "Sd"
cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
kable(t(cv_all),digits=2)
}


#Cross validation function for ksvm
cv_function_ksvm <- function(df, target, nFolds, seedVal, metrics_list, kern, c)
{
# create folds using the assigned values
set.seed(seedVal)
folds = createFolds(df[,target],nFolds)
# The lapply loop
cv_results <- lapply(folds, function(x)
{ 
# data preparation:
  test_target <- df[x,target]
  test_input <- df[x,-target]
  train_target <- df[-x,target]
  train_input <- df[-x,-target]
   pred_model <- ksvm(train_target ~ .,data = train_input,kernel=kern,C=c)  
  pred <- predict(pred_model, test_input)
  return(mmetric(test_target,pred,metrics_list))
})
cv_results_m <- as.matrix(as.data.frame(cv_results))
cv_mean<- as.matrix(rowMeans(cv_results_m))
cv_sd <- as.matrix(rowSds(cv_results_m))
colnames(cv_mean) <- "Mean"
colnames(cv_sd) <- "Sd"
cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
kable(t(cv_all),digits=2)
}


# Cross validation function for IBk
cv_IBkX <- function(df, target, nFolds, seedVal, metrics_list, k, i)
{
# create folds using the assigned values
set.seed(seedVal)
folds = createFolds(df[,target],nFolds)
# The lapply loop
cv_results <- lapply(folds, function(x)
{ 
# data preparation:

  test_target <- df[x,target]
  test_input <- df[x,-target]
  train_target <- df[-x,target]
  train_input <- df[-x,-target]
  pred_model <- IBk(train_target ~ .,data = train_input,control = Weka_control(K=k,I=i,X=TRUE))  
  pred <- predict(pred_model, test_input)
  return(mmetric(test_target,pred,metrics_list))
})
cv_results_m <- as.matrix(as.data.frame(cv_results))
cv_mean<- as.matrix(rowMeans(cv_results_m))
cv_sd <- as.matrix(rowSds(cv_results_m))
colnames(cv_mean) <- "Mean"
colnames(cv_sd) <- "Sd"
cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
kable(t(cv_all),digits=2)
}
```

# 3 fold cross-validation of MLP, ksvm and IBk models
## Use the default settings of MultilayerPerceptron, ksvm and IBk to perform cross-validation
```{r - Use the default settings to perform cross-validation, warning=FALSE, message=FALSE}
# Default cross validation for MLP
# L=0.3,M=0.2, N=500, H='a'
cv_function_MLP(df = sales_new, target = 3, 3, seedVal, metrics_list, 0.3, 0.2, 500, 0)

# Default cross validation for ksvm
cv_function_ksvm(df=sales_new, target = 3, 3, seedVal, metrics_list, 'rbfdot', 1)

# Default cross validation for IBk
cv_IBkX(df=sales_new, target = 3, 3, seedVal, metrics_list, 1, FALSE)
```

# 3 fold cross-validation of MLP, ksvm and IBk models using bank data
## Use the default settings of MultilayerPerceptron, ksvm and IBk to perform cross-validation using bank data
```{r - Use the default settings to perform cross-validation using bank data, warning=FALSE, message=FALSE}
CD_data <- CD[,-11]
df <- CD_data
target <- 20
metrics_list <- c("ACC","PRECISION","TPR","F1")
# Default cross validation for MLP
# L=0.3,M=0.2, N=500, H='a'
cv_function_MLP(df, target, 3, seedVal, metrics_list, 0.3, 0.2, 500, 0)

# Default cross validation for ksvm
cv_function_ksvm(df, target, 3, seedVal, metrics_list, 'rbfdot', 1)

# Default cross validation for IBk
cv_IBkX(df, target, 3, seedVal, metrics_list, 1, FALSE)
```
